{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 3: Regresión Lineal\n",
    "### Zuraya Huizar Cruz\n",
    "## Parte teórica\n",
    "Supongamos que queremos explicar una variable estadística $Y$ utilizando la información de $p$ variables $X^1,...,X^p$, tomando una muestra de $N$ individuos.\n",
    "Sea $X=[X^1|...|X^p]$ de tamaño $nxp$ donde cada columna es una variable y cada fila uno de los individuos de la muestra.\n",
    "\n",
    "**Veamos la regresión lineal como un problema de mínimos cuadrados** \n",
    "\n",
    "$$X\\beta=y \\hspace{1cm} con \\hspace{1cm} X= \\begin{bmatrix} X_{11} & X_{12} & \\cdots & X_{1p} \\\\ X_{21} & X_{22} & \\cdots & X_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ X_{n1} & X_{n2} & \\cdots & X_{np}  \\end{bmatrix} \\hspace{1cm} \\beta= \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p  \\end{bmatrix} \\hspace{1cm} y= \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n  \\end{bmatrix}$$\n",
    "Usualmente este sistema no tiene soluciones por lo cual se busca encontrar los coeficientes de $\\beta$ que se ajustan mejor a las ecuaciones, resolviendo el problema de mínimos cuadrados $$ \\widehat{\\beta}=argmin_{ \\beta \\in \\rm I\\!R^p} \\lVert{y-X\\beta}\\rVert ^2$$\n",
    "\n",
    "La solución teórica es $$ \\widehat{\\beta}=(X^TX)^{-1}X^Ty$$\n",
    "Este planteamiento nos da un ajuste lineal de los datos porque es equivalente a verlo como $$y=X\\beta+\\epsilon \\hspace{1cm} con \\hspace{1cm} y= \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n  \\end{bmatrix}  \\hspace{1cm} X= \\begin{bmatrix} X_{11} & X_{12} & \\cdots & X_{1p} \\\\ X_{21} & X_{22} & \\cdots & X_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ X_{n1} & X_{n2} & \\cdots & X_{np}  \\end{bmatrix} \\hspace{1cm} \\beta= \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p  \\end{bmatrix}\\hspace{1cm} \\epsilon = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n  \\end{bmatrix}\\hspace{1cm} siendo \\hspace{1cm} \\epsilon=y-X\\beta$$  \n",
    "\n",
    "También podemos usar este modelo para ajustar polinomios con el siguiente planteamiento\n",
    "$$ y=X\\beta+\\epsilon \\hspace{1cm} con \\hspace{1cm} y= \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n  \\end{bmatrix} \\hspace{1cm} X= \\begin{bmatrix} 1 & X_1 & X_1^2 & \\cdots & X_1^p \\\\ 1 & X_2 & X_2^2 & \\cdots & X_2^p \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & X_n & X_n^2 & \\cdots & X_n^p  \\end{bmatrix} \\hspace{1cm} \\beta= \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p  \\end{bmatrix} \\hspace{1cm} \\epsilon = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n  \\end{bmatrix}$$\n",
    "\n",
    "**Relación con un problema de proyección en subespacios vectoriales de álgebra lineal**  \n",
    "Cuando $\\beta X=y$  no puede resolverse exactamente se busca la aproximación de $\\beta$ que tenga el menor error. En este caso los errores con menor longitud se encuentran en la proyección ortogonal de y sobre el espacio generado por las columnas de X por lo cual en realidad estamos resolviendo $$\\beta X=Proy_{span(X^1,...,X^p)}(y)$$\n",
    "![Proyección ortogonal](https://upload.wikimedia.org/wikipedia/commons/8/87/OLS_geometric_interpretation.svg)\n",
    "\n",
    "**¿Cuál es la relación particular con el teorema de Pitágoras?**  \n",
    "El teorema de Pytágoras dice que *\"En todo triángulo rectángulo el cuadrado de la hipotenusa es igual a la suma de los cuadrados de los catetos\"*. Lo cual se puede aplicar por la proyección ortogonal de la siguiente manera $$\\lVert{X\\beta}\\rVert^2+\\lVert{y-X\\beta}\\rVert^2=\\lVert{y}\\rVert^2$$\n",
    "\n",
    "**¿Qué logramos al agregar una columna de unos en la matriz $X$?** Es decir, definir mejor $X=[1_n|X^1|...|X^p]$ con $1_n=1,1,...,1]^T$  \n",
    "Al agregar la columna de unos conseguimos integrar el coeficiente de las constantes $\\beta_0$\n",
    "$$ y=X\\beta+\\epsilon \\hspace{1cm} es \\ decir \\hspace{1cm} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n  \\end{bmatrix} = \\begin{bmatrix} \\beta_0 +\\beta_1*X_{11}+\\beta_2*X_{12}+ \\cdots + \\beta_p*X_{1p} +\\epsilon_1 \\\\ \\beta_0 +\\beta_1*X_{21}+\\beta_2*X_{22}+ \\cdots + \\beta_p*X_{2p} +\\epsilon_2\\\\  \\vdots \\\\ \\beta_0 +\\beta_1*X_{n1}+\\beta_2*X_{n2}+ \\cdots + \\beta_p*X_{np} + \\epsilon_n \\end{bmatrix} \\hspace{1cm}$$\n",
    "De forma similar ocurre en el caso de los polinomios\n",
    "$$ y=X\\beta+\\epsilon \\hspace{1cm} es \\ decir \\hspace{1cm} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n  \\end{bmatrix} = \\begin{bmatrix} \\beta_0 +\\beta_1*X_1+\\beta_2*X_1^2+ \\cdots + \\beta_p*X_1^p +\\epsilon_1 \\\\ \\beta_0 +\\beta_1*X_2+\\beta_2*X_2^2+ \\cdots + \\beta_p*X_2^p +\\epsilon_2\\\\  \\vdots \\\\ \\beta_0 +\\beta_1*X_n+\\beta_2*X_n^2+ \\cdots + \\beta_p*X_n^p + \\epsilon_n \\end{bmatrix} \\hspace{1cm}$$\n",
    "\n",
    "**¿Cuál es la función de verosimilitud del problema anterior?**\n",
    "   $Y=X\\beta+\\epsilon$ con $\\epsilon \\sim N(0,\\sigma^2I_n)$ con $I_n$ la matriz \n",
    "   y concluir que $Y \\sim N(X \\beta, \\sigma^2I_n)$ y escriban entonces $L(\\beta, \\sigma^2) = f(Y|\\beta, \\sigma^2,X)$.  \n",
    "La función de verosimilitud es $$L(Y_1,...Y_n;\\beta,\\sigma^2)=\\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}} \\exp \\biggl(\\frac{-1}{2\\sigma^2}\\biggl(\\sum_{i=1}^{n}(Y_i-\\beta X_i)^2\\biggr)\\biggr)$$  \n",
    "\n",
    "**Mostrar que la solución de máxima verosimilitud es la misma que la del problema de mínimos cuadrados**\n",
    "Para encontrar el máximo podemos obtener las derivadas de la función logaritmica que es estrictamente creciente  \n",
    "$$\\log \\biggl(L(Y_1,...Y_n;\\beta,\\sigma^2)\\biggr)=\\log \\biggl( \\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}} \\exp \\biggl(\\frac{-1}{2\\sigma^2}\\biggl(\\sum_{i=1}^{n}(Y_i-\\beta X_i)^2\\biggr)\\biggr) \\biggr)=-n\\log \\sqrt{2\\pi\\sigma^2}-\\frac{1}{2\\sigma^2}\\biggl(\\sum_{i=1}^{n}(Y_i-\\beta X_i)^2\\biggr)$$\n",
    "$$\\frac{\\partial L}{\\partial \\beta}=\\frac{2}{2\\sigma^2}\\biggl(\\sum_{i=1}^{n} X_i(Y_i-\\beta X_i)^2\\biggr)=0$$\n",
    "Maximizar la verosimilitud es equivalente a minimizar $$\\sum_{i=1}^{n}(Y_i-\\beta X_i)^2$$ que es justamente el problema de mínimos cuadrados.  \n",
    "\n",
    "**Investiga el contenido del Teorema de Gauss-Markov sobre minimos cuadrados.**  \n",
    "Sea $$y=X\\beta+\\epsilon$$ con  \n",
    "$$E(\\epsilon \\ |X)=0 \\ \\forall \\ X \\ (errores \\ con \\ media \\ cero)$$\n",
    "$$Var(\\epsilon \\ |X)=\\sigma_\\epsilon^2 \\ I_N \\ (homocedasticidad)$$\n",
    "El teorema Gauss-Markov dice que el óptimo estimador lineal insesgado de mínima varianza es\n",
    "$ \\widehat{\\beta}=(X^TX)^{-1} X^Ty$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
